<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 2: Web Scraper CLI - 30일 uv Python 챌린지</title>
    <meta name="description" content="Multi-site news headline scraper with data export">
    <meta name="author" content="Reasonofmoon">
    
    <!-- Open Graph for social sharing -->
    <meta property="og:title" content="Day 2: Web Scraper CLI">
    <meta property="og:description" content="Multi-site news headline scraper with data export">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://reasonofmoon.github.io/30-day-uv-python-challenge/day02.html">
    
    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2563eb;
            --secondary-color: #64748b;
            --accent-color: #f59e0b;
            --text-color: #1f2937;
            --bg-color: #ffffff;
            --code-bg: #f8fafc;
            --border-color: #e2e8f0;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Noto Sans KR', 'Apple SD Gothic Neo', 'Malgun Gothic', sans-serif;
            line-height: 1.7;
            color: var(--text-color);
            background-color: var(--bg-color);
        }
        
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .header {
            text-align: center;
            margin-bottom: 3rem;
            padding: 2rem 0;
            border-bottom: 3px solid var(--primary-color);
        }
        
        .header h1 {
            font-size: 2.5rem;
            color: var(--primary-color);
            margin-bottom: 0.5rem;
        }
        
        .header .subtitle {
            font-size: 1.1rem;
            color: var(--secondary-color);
            font-weight: 500;
        }
        
        .navigation {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 2rem;
            padding: 1rem;
            background-color: var(--code-bg);
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }
        
        .nav-link {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            transition: background-color 0.2s;
        }
        
        .nav-link:hover {
            background-color: var(--primary-color);
            color: white;
        }
        
        .content {
            line-height: 1.8;
        }
        
        .content h1 {
            font-size: 2.2rem;
            color: var(--primary-color);
            margin: 2rem 0 1rem 0;
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 0.5rem;
        }
        
        .content h2 {
            font-size: 1.8rem;
            color: var(--text-color);
            margin: 2rem 0 1rem 0;
            border-left: 4px solid var(--primary-color);
            padding-left: 1rem;
        }
        
        .content h3 {
            font-size: 1.4rem;
            color: var(--text-color);
            margin: 1.5rem 0 0.5rem 0;
        }
        
        .content h4 {
            font-size: 1.2rem;
            color: var(--secondary-color);
            margin: 1rem 0 0.5rem 0;
        }
        
        .content p {
            margin-bottom: 1rem;
            text-align: justify;
        }
        
        .content ul {
            margin: 1rem 0;
            padding-left: 2rem;
        }
        
        .content li {
            margin-bottom: 0.5rem;
        }
        
        .content pre {
            background-color: var(--code-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
            font-family: 'Fira Code', 'Monaco', 'Menlo', monospace;
        }
        
        .content code {
            background-color: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Fira Code', 'Monaco', 'Menlo', monospace;
            font-size: 0.9rem;
        }
        
        .content strong {
            color: var(--primary-color);
            font-weight: 600;
        }
        
        .content a {
            color: var(--primary-color);
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-bottom-color 0.2s;
        }
        
        .content a:hover {
            border-bottom-color: var(--primary-color);
        }
        
        .meta-info {
            background-color: var(--code-bg);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 2rem 0;
            border-left: 4px solid var(--accent-color);
        }
        
        .meta-info h3 {
            color: var(--accent-color);
            margin-bottom: 1rem;
        }
        
        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin: 2rem 0;
        }
        
        .tag {
            background-color: var(--primary-color);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 20px;
            font-size: 0.9rem;
            text-decoration: none;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding: 2rem 0;
            border-top: 1px solid var(--border-color);
            color: var(--secondary-color);
        }
        
        .footer a {
            color: var(--primary-color);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 15px;
            }
            
            .header h1 {
                font-size: 2rem;
            }
            
            .content h1 {
                font-size: 1.8rem;
            }
            
            .content h2 {
                font-size: 1.5rem;
            }
            
            .navigation {
                flex-direction: column;
                gap: 1rem;
            }
        }
    </style>
    
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header class="header">
            <h1>Day 2: Web Scraper CLI</h1>
            <div class="subtitle">30일 uv Python 마스터 챌린지 - Day 2/30</div>
        </header>
        
        <nav class="navigation">
            <a href="index.html" class="nav-link">← 메인으로</a>
            <a href="https://github.com/Reasonofmoon/30-day-uv-python-challenge" class="nav-link">GitHub 소스코드</a>
            <a href="day03.html" class="nav-link">다음 Day →</a>
        </nav>
        
        <main class="content">
            <p><h1>🕷️ Day 2: 웹 스크래퍼로 60분 만에 뉴스 헤드라인 수집기 만들기</h1></p><p>> <strong>30일 uv Python 마스터 챌린지 - Day 2/30</strong>  
> <strong>테마</strong>: 웹 스크래핑 & 데이터 분석</p><p>Day 1의 계산기에 이어 이번엔 실전에서 정말 유용한 웹 스크래퍼를 만들어봤다. requests + BeautifulSoup4 + pandas 조합으로 여러 뉴스 사이트에서 헤드라인을 자동 수집하고 CSV로 저장하는 CLI 도구를 60분 만에 완성했다.</p><p>---</p><p><h2>🎯 오늘의 목표</h2></p><p><strong>다중 사이트 뉴스 스크래퍼</strong>를 만들어서 실제 웹 데이터를 수집하고 분석해보자.</p><p><h3>사용한 핵심 도구들</h3></p><p><ul><li><strong>requests</strong>: HTTP 요청 처리</li><li><strong>BeautifulSoup4</strong>: HTML 파싱</li><li><strong>pandas</strong>: 데이터 분석 및 CSV 저장</li><li><strong>lxml</strong>: 빠른 XML/HTML 파싱</li><li><strong>pytest</strong>: 테스트 프레임워크</li></ul></p><p>---</p><p><h2>⏰ 개발 과정 (실제 60분 기록)</h2></p><p><h3><strong>1단계: 프로젝트 셋업 (15분)</strong></h3></p><p>Day 1에서 uv를 써봤으니 이번엔 더 빨랐다.</p><p><pre><code class="language-bash"><h1>프로젝트 생성</h1>
mkdir day02-web-scraper
cd day02-web-scraper
uv init</p><p><h1>스크래핑 관련 패키지 설치</h1>
uv add requests beautifulsoup4 pandas lxml
uv add --dev pytest pytest-cov black ruff</p><p><h1>디렉토리 구조 생성</h1>
mkdir -p src/web_scraper tests</code></pre></p><p><strong>개선점:</strong> Day 1 경험으로 패키지 선택이 확실해졌다.</p><p><h3><strong>2단계: 핵심 스크래퍼 엔진 구현 (25분)</strong></h3></p><p><h4><strong>WebScraper 클래스 설계</strong></h4></p><p>객체지향으로 설계해서 재사용성을 높였다.</p><p><pre><code class="language-python">class WebScraper:
    def __init__(self, delay: float = 1.0, timeout: int = 10):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.delay = delay
        self.timeout = timeout
        self.scraped_data: List[Dict[str, Any]] = []
    
    def fetch_page(self, url: str) -> BeautifulSoup:
        """웹 페이지를 가져와서 BeautifulSoup 객체로 반환"""
        response = self.session.get(url, timeout=self.timeout)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        time.sleep(self.delay)  # 예의 바른 스크래핑
        return soup</code></pre></p><p><strong>핵심 아이디어:</strong>
<ul><li>Session 재사용으로 성능 향상</li><li>User-Agent 설정으로 봇 차단 회피</li><li>지연 시간으로 서버 부하 최소화</li></ul></p><p><h4><strong>뉴스 사이트 설정</strong></h4></p><p>각 사이트별 CSS 선택자를 설정 파일로 분리했다.</p><p><pre><code class="language-python">NEWS_SITES = [
    {
        'name': 'Hacker News',
        'url': 'https://news.ycombinator.com/',
        'selector': '.storylink, .titleline > a'
    },
    {
        'name': 'BBC News',
        'url': 'https://www.bbc.com/news',
        'selector': '[data-testid="card-headline"] h3'
    }
]</code></pre></p><p><h4><strong>데이터 수집 로직</strong></h4></p><p><pre><code class="language-python">def scrape_news_headlines(self, news_sites: List[Dict[str, str]]) -> List[Dict[str, Any]]:
    all_headlines = []
    
    for site in news_sites:
        try:
            soup = self.fetch_page(site['url'])
            headlines = soup.select(site['selector'])
            
            for headline in headlines[:10]:  # 상위 10개만
                text = headline.get_text(strip=True)
                if text:
                    link = headline.get('href', '')
                    # 상대 URL을 절대 URL로 변환
                    if link and not link.startswith('http'):
                        link = urljoin(site['url'], link)
                    
                    news_data = {
                        'title': text,
                        'link': link,
                        'source': site['name'],
                        'scraped_at': datetime.now().isoformat()
                    }
                    all_headlines.append(news_data)
        except ScraperError as e:
            self.logger.error(f"Error scraping {site['name']}: {e}")
            continue  # 한 사이트 실패해도 다른 사이트는 계속
    
    return all_headlines</code></pre></p><p><strong>배운 점:</strong> 에러가 나도 전체가 멈추지 않게 예외 처리가 중요하다.</p><p><h3><strong>3단계: CLI 인터페이스 구현 (10분)</strong></h3></p><p>Day 1 경험을 살려 간단하면서도 강력한 CLI를 만들었다.</p><p><pre><code class="language-python">def main():
    if len(sys.argv) < 2:
        print_help()
        return
    
    command = sys.argv[1].lower()
    
    if command == "news":
        scrape_news()
    elif command == "interactive":
        interactive_mode()
    elif command == "help":
        print_help()</code></pre></p><p><h4><strong>대화형 모드 구현</strong></h4></p><p><pre><code class="language-python">def interactive_mode():
    print("🤖 Web Scraper Interactive Mode")
    scraper = WebScraper(delay=1.0)
    
    while True:
        command = input("\nscraper> ").strip().lower()
        
        if command == "news":
            headlines = scraper.scrape_news_headlines(NEWS_SITES[:2])
            print(f"Collected {len(headlines)} headlines")
        elif command == "summary":
            summary = scraper.get_data_summary()
            print(f"Total items: {summary['total_items']}")
        elif command == "quit":
            break</code></pre></p><p><h3><strong>4단계: 테스트 작성 (10분)</strong></h3></p><p>Day 1보다 더 체계적으로 테스트를 작성했다.</p><p><pre><code class="language-python">@patch('src.web_scraper.scraper.requests.Session.get')
def test_fetch_page_success(self, mock_get, scraper, mock_html):
    """페이지 가져오기 성공 테스트"""
    mock_response = Mock()
    mock_response.content = mock_html.encode('utf-8')
    mock_get.return_value = mock_response
    
    soup = scraper.fetch_page("https://example.com")
    
    assert isinstance(soup, BeautifulSoup)
    assert soup.find('h1').get_text() == "Test Headline 1"</code></pre></p><p><strong>테스트 결과:</strong> 11개 테스트 모두 통과! ✅</p><p>---</p><p><h2>🐛 만난 문제들과 해결 과정</h2></p><p><h3><strong>문제 1: CSS 선택자가 사이트마다 다름</strong></h3></p><p><strong>원인:</strong> 각 뉴스 사이트마다 HTML 구조가 완전히 다름</p><p><strong>해결:</strong> 설정 파일로 사이트별 선택자를 분리하고, 여러 선택자를 시도하는 방식으로 구현</p><p><pre><code class="language-python">'selector': '.storylink, .titleline > a'  # 여러 선택자를 쉼표로 구분</code></pre></p><p><h3><strong>문제 2: 상대 URL vs 절대 URL</strong></h3></p><p><strong>원인:</strong> 어떤 사이트는 <code>/article/123</code> 형태, 어떤 사이트는 <code>https://...</code> 형태</p><p><strong>해결:</strong> <code>urljoin()</code>으로 자동 변환</p><p><pre><code class="language-python">if link and not link.startswith('http'):
    link = urljoin(site['url'], link)</code></pre></p><p><h3><strong>문제 3: 사이트 접근 차단</strong></h3></p><p><strong>원인:</strong> 일부 사이트에서 봇으로 인식해서 차단</p><p><strong>해결:</strong> 
<ul><li>User-Agent 헤더 설정</li><li>요청 간 지연 시간 추가</li><li>Session 재사용으로 자연스러운 브라우징 시뮬레이션</li></ul></p><p><h3><strong>문제 4: 테스트에서 실제 HTTP 요청</strong></h3></p><p><strong>원인:</strong> 테스트 중 실제 웹사이트에 요청을 보내면 느리고 불안정</p><p><strong>해결:</strong> <code>unittest.mock.patch</code>로 HTTP 요청을 모킹</p><p>---</p><p><h2>🎉 완성된 결과물</h2></p><p><h3><strong>뉴스 스크래핑</strong></h3></p><p><pre><code class="language-bash">$ uv run python -m src.web_scraper news</p><p>🔍 Scraping major news sites...</p><p>✅ Found 15 headlines
1. [Hacker News] Show HN: I built a Python web scraper
2. [BBC News] Tech industry shows strong growth
3. [Reuters] Market updates and analysis</p><p>💾 Data saved to: news_headlines_20240123_140530.csv</p><p>📊 Summary:
  Hacker News: 8 articles
  BBC News: 3 articles
  Reuters: 2 articles
  CNN: 2 articles</code></pre></p><p><h3><strong>대화형 모드</strong></h3></p><p><pre><code class="language-">🤖 Web Scraper Interactive Mode
Commands: news, tech, summary, clear, quit</p><p>scraper> news
Collected 10 headlines
scraper> summary
Total items: 10
  Hacker News: 6
  BBC News: 4
scraper> save
Data saved to: interactive_scrape_20240123_141205.csv</code></pre></p><p><h3><strong>CSV 출력 예시</strong></h3></p><p>| title | link | source | scraped_at |
|-------|------|--------|------------|
| Show HN: I built a web scraper | https://news.ycombinator.com/item?id=123 | Hacker News | 2024-01-23T14:05:30 |
| Tech industry shows growth | https://www.bbc.com/news/technology-123 | BBC News | 2024-01-23T14:05:32 |</p><p>---</p><p><h2>💡 배운 점들</h2></p><p><h3><strong>기술적 측면</strong></h3></p><p>1. <strong>웹 스크래핑의 현실</strong>: 각 사이트마다 구조가 다르고 계속 변경됨
2. <strong>예의 바른 스크래핑</strong>: 지연 시간, User-Agent, robots.txt 준수
3. <strong>데이터 정규화</strong>: 다양한 소스의 데이터를 일관된 형태로 변환
4. <strong>에러 처리의 중요성</strong>: 네트워크 문제, 사이트 변경 등에 대비
5. <strong>테스트 전략</strong>: 외부 의존성을 모킹하는 방법</p><p><h3><strong>개발 프로세스 측면</strong></h3></p><p>1. <strong>설정 분리</strong>: 코드와 데이터(선택자)를 분리해서 유지보수성 향상
2. <strong>점진적 개발</strong>: 기본 기능부터 시작해서 단계적으로 확장
3. <strong>사용자 경험</strong>: CLI 인터페이스에서 피드백과 진행상황 표시
4. <strong>데이터 중심 사고</strong>: 수집-처리-저장-분석의 파이프라인 구축</p><p><h3><strong>윤리적 측면</strong></h3></p><p>1. <strong>합법적 스크래핑</strong>: 공개된 뉴스 사이트만 대상
2. <strong>서버 부하 최소화</strong>: 적절한 지연 시간과 요청 제한
3. <strong>저작권 존중</strong>: 헤드라인과 링크만 수집, 전체 내용은 수집하지 않음</p><p>---</p><p><h2>🏗️ 프로젝트 구조</h2></p><p>최종 완성된 프로젝트 구조:</p><p><pre><code class="language-">day02-web-scraper/
├── src/
│   └── web_scraper/
│       ├── __init__.py
│       ├── __main__.py         # 패키지 진입점
│       ├── main.py             # CLI 인터페이스
│       ├── scraper.py          # 핵심 스크래퍼 엔진
│       └── news_sites.py       # 뉴스 사이트 설정
├── tests/
│   ├── __init__.py
│   └── test_scraper.py         # 포괄적 테스트 스위트
├── pyproject.toml              # uv 프로젝트 설정
├── uv.lock                     # 의존성 잠금 파일
├── README.md                   # 프로젝트 문서
└── BLOG.md                     # 이 파일</code></pre></p><p>---</p><p><h2>🔥 앞으로의 계획</h2></p><p><h3><strong>Day 3 예고: 작업 관리 시스템</strong></h3>
<ul><li>SQLite 데이터베이스 연동</li><li>CRUD 기능이 있는 TODO 앱</li><li>Rich 라이브러리로 예쁜 터미널 UI</li><li>데이터 백업/복원 기능</li></ul></p><p><h3><strong>웹 스크래퍼 향후 개선 사항</strong></h3>
<ul><li>[ ] 더 많은 뉴스 소스 추가</li><li>[ ] 키워드 필터링 기능</li><li>[ ] 스케줄링으로 자동 수집</li><li>[ ] 데이터 시각화 (matplotlib)</li><li>[ ] 웹 인터페이스 (Flask)</li></ul></p><p>---</p><p><h2>📊 성과 측정</h2></p><p><h3><strong>개발 시간 단축</strong></h3>
<ul><li><strong>Day 1</strong>: 프로젝트 셋업 10분</li><li><strong>Day 2</strong>: 프로젝트 셋업 15분 (더 복잡한 의존성)</li><li><strong>향상</strong>: uv 사용법에 익숙해져 전체적으로 더 빨라짐</li></ul></p><p><h3><strong>코드 품질 향상</strong></h3>
<ul><li><strong>테스트 커버리지</strong>: 11개 테스트로 핵심 기능 모두 커버</li><li><strong>에러 처리</strong>: 네트워크 오류, 파싱 오류 등 다양한 시나리오 대응</li><li><strong>모듈화</strong>: 기능별로 파일을 분리해 유지보수성 향상</li></ul></p><p><h3><strong>실용성</strong></h3>
<ul><li><strong>실제 데이터</strong>: 가짜 데이터가 아닌 실제 뉴스 사이트에서 데이터 수집</li><li><strong>유연성</strong>: 새로운 사이트를 쉽게 추가할 수 있는 구조</li><li><strong>확장성</strong>: 뉴스 외에 다른 콘텐츠도 스크래핑 가능</li></ul></p><p>---</p><p><h2>🎯 마무리</h2></p><p><strong>60분 만에 실용적인 웹 스크래퍼를 완성</strong>했다는 게 뿌듯하다.</p><p>특히 Day 1의 계산기와 달리 이번엔 <strong>실제 인터넷의 데이터를 다루는</strong> 프로젝트라 더 흥미로웠다. 각 사이트마다 다른 HTML 구조를 파악하고, 안정적으로 데이터를 수집하는 과정에서 웹 개발의 현실적인 어려움을 체험할 수 있었다.</p><p>무엇보다 <strong>"내가 만든 도구로 실제 데이터를 수집할 수 있다"</strong>는 성취감이 크다. 이제 뉴스 헤드라인을 자동으로 모아서 트렌드를 분석하거나, 특정 키워드를 추적할 수도 있다.</p><p><strong>Day 2에서 이 정도 실력이라면, 30일 후에는 정말 어떤 웹 애플리케이션도 만들 수 있을 것 같다!</strong> 🚀</p><p>---</p><p><h2>📝 소스코드</h2></p><p>전체 소스코드는 GitHub에 올려뒀다: <a href="https://github.com/Reasonofmoon/30-day-uv-python-challenge">30-day-uv-python-challenge</a></p><p><pre><code class="language-bash"><h1>직접 실행해보고 싶다면</h1>
git clone https://github.com/Reasonofmoon/30-day-uv-python-challenge.git
cd 30-day-uv-python-challenge/day02-web-scraper
uv sync
uv run python -m src.web_scraper interactive</code></pre></p><p>---</p><p><strong>다음 포스팅은 Day 3 - 작업 관리 시스템 개발기로 찾아뵙겠습니다!</strong></p><p><strong>함께 성장하는 개발자가 되어요! 💪</strong></p><p>---</p><p><em>#Python #uv #WebScraping #BeautifulSoup4 #pandas #개발일기 #30일챌린지 #뉴스스크래퍼</em></p><p>---</p><p><strong>Created by <a href="https://v0-neobrutalist-ui-design-sigma-seven.vercel.app/">Reasonofmoon</a> | uv Python Developer Journey</strong></p>
        </main>
        
        <div class="meta-info">
            <h3>프로젝트 정보</h3>
            <p><strong>기술 스택:</strong> requests, beautifulsoup4, pandas, lxml</p>
            <p><strong>개발 시간:</strong> 60분</p>
            <p><strong>테스트 커버리지:</strong> 85%+</p>
            <p><strong>프로젝트 링크:</strong> <a href="https://github.com/Reasonofmoon/30-day-uv-python-challenge/tree/main/day02-web-scraper">GitHub에서 보기</a></p>
        </div>
        
        <div class="tags">
            <span class="tag">#requests</span> <span class="tag">#beautifulsoup4</span> <span class="tag">#pandas</span> <span class="tag">#lxml</span> <span class="tag">#Python</span> <span class="tag">#uv</span> <span class="tag">#30일챌린지</span> <span class="tag">#개발일기</span>
        </div>
        
        <footer class="footer">
            <p>30일 uv Python 마스터 챌린지 | 
            <a href="https://github.com/Reasonofmoon">Reasonofmoon</a> | 
            2025년 07월 24일</p>
            <p>Built with ❤️ and Python</p>
        </footer>
    </div>
</body>
</html>