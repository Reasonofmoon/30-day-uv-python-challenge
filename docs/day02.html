<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 2: Web Scraper CLI - 30ì¼ uv Python ì±Œë¦°ì§€</title>
    <meta name="description" content="Multi-site news headline scraper with data export">
    <meta name="author" content="Reasonofmoon">
    
    <!-- Open Graph for social sharing -->
    <meta property="og:title" content="Day 2: Web Scraper CLI">
    <meta property="og:description" content="Multi-site news headline scraper with data export">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://reasonofmoon.github.io/30-day-uv-python-challenge/day02.html">
    
    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2563eb;
            --secondary-color: #64748b;
            --accent-color: #f59e0b;
            --text-color: #1f2937;
            --bg-color: #ffffff;
            --code-bg: #f8fafc;
            --border-color: #e2e8f0;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Noto Sans KR', 'Apple SD Gothic Neo', 'Malgun Gothic', sans-serif;
            line-height: 1.7;
            color: var(--text-color);
            background-color: var(--bg-color);
        }
        
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .header {
            text-align: center;
            margin-bottom: 3rem;
            padding: 2rem 0;
            border-bottom: 3px solid var(--primary-color);
        }
        
        .header h1 {
            font-size: 2.5rem;
            color: var(--primary-color);
            margin-bottom: 0.5rem;
        }
        
        .header .subtitle {
            font-size: 1.1rem;
            color: var(--secondary-color);
            font-weight: 500;
        }
        
        .navigation {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 2rem;
            padding: 1rem;
            background-color: var(--code-bg);
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }
        
        .nav-link {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            transition: background-color 0.2s;
        }
        
        .nav-link:hover {
            background-color: var(--primary-color);
            color: white;
        }
        
        .content {
            line-height: 1.8;
        }
        
        .content h1 {
            font-size: 2.2rem;
            color: var(--primary-color);
            margin: 2rem 0 1rem 0;
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 0.5rem;
        }
        
        .content h2 {
            font-size: 1.8rem;
            color: var(--text-color);
            margin: 2rem 0 1rem 0;
            border-left: 4px solid var(--primary-color);
            padding-left: 1rem;
        }
        
        .content h3 {
            font-size: 1.4rem;
            color: var(--text-color);
            margin: 1.5rem 0 0.5rem 0;
        }
        
        .content h4 {
            font-size: 1.2rem;
            color: var(--secondary-color);
            margin: 1rem 0 0.5rem 0;
        }
        
        .content p {
            margin-bottom: 1rem;
            text-align: justify;
        }
        
        .content ul {
            margin: 1rem 0;
            padding-left: 2rem;
        }
        
        .content li {
            margin-bottom: 0.5rem;
        }
        
        .content pre {
            background-color: var(--code-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
            font-family: 'Fira Code', 'Monaco', 'Menlo', monospace;
        }
        
        .content code {
            background-color: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Fira Code', 'Monaco', 'Menlo', monospace;
            font-size: 0.9rem;
        }
        
        .content strong {
            color: var(--primary-color);
            font-weight: 600;
        }
        
        .content a {
            color: var(--primary-color);
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-bottom-color 0.2s;
        }
        
        .content a:hover {
            border-bottom-color: var(--primary-color);
        }
        
        .meta-info {
            background-color: var(--code-bg);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 2rem 0;
            border-left: 4px solid var(--accent-color);
        }
        
        .meta-info h3 {
            color: var(--accent-color);
            margin-bottom: 1rem;
        }
        
        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin: 2rem 0;
        }
        
        .tag {
            background-color: var(--primary-color);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 20px;
            font-size: 0.9rem;
            text-decoration: none;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding: 2rem 0;
            border-top: 1px solid var(--border-color);
            color: var(--secondary-color);
        }
        
        .footer a {
            color: var(--primary-color);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 15px;
            }
            
            .header h1 {
                font-size: 2rem;
            }
            
            .content h1 {
                font-size: 1.8rem;
            }
            
            .content h2 {
                font-size: 1.5rem;
            }
            
            .navigation {
                flex-direction: column;
                gap: 1rem;
            }
        }
    </style>
    
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header class="header">
            <h1>Day 2: Web Scraper CLI</h1>
            <div class="subtitle">30ì¼ uv Python ë§ˆìŠ¤í„° ì±Œë¦°ì§€ - Day 2/30</div>
        </header>
        
        <nav class="navigation">
            <a href="index.html" class="nav-link">â† ë©”ì¸ìœ¼ë¡œ</a>
            <a href="https://github.com/Reasonofmoon/30-day-uv-python-challenge" class="nav-link">GitHub ì†ŒìŠ¤ì½”ë“œ</a>
            <a href="day03.html" class="nav-link">ë‹¤ìŒ Day â†’</a>
        </nav>
        
        <main class="content">
            <p><h1>ğŸ•·ï¸ Day 2: ì›¹ ìŠ¤í¬ë˜í¼ë¡œ 60ë¶„ ë§Œì— ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘ê¸° ë§Œë“¤ê¸°</h1></p><p>> <strong>30ì¼ uv Python ë§ˆìŠ¤í„° ì±Œë¦°ì§€ - Day 2/30</strong>  
> <strong>í…Œë§ˆ</strong>: ì›¹ ìŠ¤í¬ë˜í•‘ & ë°ì´í„° ë¶„ì„</p><p>Day 1ì˜ ê³„ì‚°ê¸°ì— ì´ì–´ ì´ë²ˆì—” ì‹¤ì „ì—ì„œ ì •ë§ ìœ ìš©í•œ ì›¹ ìŠ¤í¬ë˜í¼ë¥¼ ë§Œë“¤ì–´ë´¤ë‹¤. requests + BeautifulSoup4 + pandas ì¡°í•©ìœ¼ë¡œ ì—¬ëŸ¬ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ í—¤ë“œë¼ì¸ì„ ìë™ ìˆ˜ì§‘í•˜ê³  CSVë¡œ ì €ì¥í•˜ëŠ” CLI ë„êµ¬ë¥¼ 60ë¶„ ë§Œì— ì™„ì„±í–ˆë‹¤.</p><p>---</p><p><h2>ğŸ¯ ì˜¤ëŠ˜ì˜ ëª©í‘œ</h2></p><p><strong>ë‹¤ì¤‘ ì‚¬ì´íŠ¸ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼</strong>ë¥¼ ë§Œë“¤ì–´ì„œ ì‹¤ì œ ì›¹ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•´ë³´ì.</p><p><h3>ì‚¬ìš©í•œ í•µì‹¬ ë„êµ¬ë“¤</h3></p><p><ul><li><strong>requests</strong>: HTTP ìš”ì²­ ì²˜ë¦¬</li><li><strong>BeautifulSoup4</strong>: HTML íŒŒì‹±</li><li><strong>pandas</strong>: ë°ì´í„° ë¶„ì„ ë° CSV ì €ì¥</li><li><strong>lxml</strong>: ë¹ ë¥¸ XML/HTML íŒŒì‹±</li><li><strong>pytest</strong>: í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬</li></ul></p><p>---</p><p><h2>â° ê°œë°œ ê³¼ì • (ì‹¤ì œ 60ë¶„ ê¸°ë¡)</h2></p><p><h3><strong>1ë‹¨ê³„: í”„ë¡œì íŠ¸ ì…‹ì—… (15ë¶„)</strong></h3></p><p>Day 1ì—ì„œ uvë¥¼ ì¨ë´¤ìœ¼ë‹ˆ ì´ë²ˆì—” ë” ë¹¨ëë‹¤.</p><p><pre><code class="language-bash"><h1>í”„ë¡œì íŠ¸ ìƒì„±</h1>
mkdir day02-web-scraper
cd day02-web-scraper
uv init</p><p><h1>ìŠ¤í¬ë˜í•‘ ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜</h1>
uv add requests beautifulsoup4 pandas lxml
uv add --dev pytest pytest-cov black ruff</p><p><h1>ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±</h1>
mkdir -p src/web_scraper tests</code></pre></p><p><strong>ê°œì„ ì :</strong> Day 1 ê²½í—˜ìœ¼ë¡œ íŒ¨í‚¤ì§€ ì„ íƒì´ í™•ì‹¤í•´ì¡Œë‹¤.</p><p><h3><strong>2ë‹¨ê³„: í•µì‹¬ ìŠ¤í¬ë˜í¼ ì—”ì§„ êµ¬í˜„ (25ë¶„)</strong></h3></p><p><h4><strong>WebScraper í´ë˜ìŠ¤ ì„¤ê³„</strong></h4></p><p>ê°ì²´ì§€í–¥ìœ¼ë¡œ ì„¤ê³„í•´ì„œ ì¬ì‚¬ìš©ì„±ì„ ë†’ì˜€ë‹¤.</p><p><pre><code class="language-python">class WebScraper:
    def __init__(self, delay: float = 1.0, timeout: int = 10):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.delay = delay
        self.timeout = timeout
        self.scraped_data: List[Dict[str, Any]] = []
    
    def fetch_page(self, url: str) -> BeautifulSoup:
        """ì›¹ í˜ì´ì§€ë¥¼ ê°€ì ¸ì™€ì„œ BeautifulSoup ê°ì²´ë¡œ ë°˜í™˜"""
        response = self.session.get(url, timeout=self.timeout)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        time.sleep(self.delay)  # ì˜ˆì˜ ë°”ë¥¸ ìŠ¤í¬ë˜í•‘
        return soup</code></pre></p><p><strong>í•µì‹¬ ì•„ì´ë””ì–´:</strong>
<ul><li>Session ì¬ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ</li><li>User-Agent ì„¤ì •ìœ¼ë¡œ ë´‡ ì°¨ë‹¨ íšŒí”¼</li><li>ì§€ì—° ì‹œê°„ìœ¼ë¡œ ì„œë²„ ë¶€í•˜ ìµœì†Œí™”</li></ul></p><p><h4><strong>ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì„¤ì •</strong></h4></p><p>ê° ì‚¬ì´íŠ¸ë³„ CSS ì„ íƒìë¥¼ ì„¤ì • íŒŒì¼ë¡œ ë¶„ë¦¬í–ˆë‹¤.</p><p><pre><code class="language-python">NEWS_SITES = [
    {
        'name': 'Hacker News',
        'url': 'https://news.ycombinator.com/',
        'selector': '.storylink, .titleline > a'
    },
    {
        'name': 'BBC News',
        'url': 'https://www.bbc.com/news',
        'selector': '[data-testid="card-headline"] h3'
    }
]</code></pre></p><p><h4><strong>ë°ì´í„° ìˆ˜ì§‘ ë¡œì§</strong></h4></p><p><pre><code class="language-python">def scrape_news_headlines(self, news_sites: List[Dict[str, str]]) -> List[Dict[str, Any]]:
    all_headlines = []
    
    for site in news_sites:
        try:
            soup = self.fetch_page(site['url'])
            headlines = soup.select(site['selector'])
            
            for headline in headlines[:10]:  # ìƒìœ„ 10ê°œë§Œ
                text = headline.get_text(strip=True)
                if text:
                    link = headline.get('href', '')
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if link and not link.startswith('http'):
                        link = urljoin(site['url'], link)
                    
                    news_data = {
                        'title': text,
                        'link': link,
                        'source': site['name'],
                        'scraped_at': datetime.now().isoformat()
                    }
                    all_headlines.append(news_data)
        except ScraperError as e:
            self.logger.error(f"Error scraping {site['name']}: {e}")
            continue  # í•œ ì‚¬ì´íŠ¸ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ ì‚¬ì´íŠ¸ëŠ” ê³„ì†
    
    return all_headlines</code></pre></p><p><strong>ë°°ìš´ ì :</strong> ì—ëŸ¬ê°€ ë‚˜ë„ ì „ì²´ê°€ ë©ˆì¶”ì§€ ì•Šê²Œ ì˜ˆì™¸ ì²˜ë¦¬ê°€ ì¤‘ìš”í•˜ë‹¤.</p><p><h3><strong>3ë‹¨ê³„: CLI ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ (10ë¶„)</strong></h3></p><p>Day 1 ê²½í—˜ì„ ì‚´ë ¤ ê°„ë‹¨í•˜ë©´ì„œë„ ê°•ë ¥í•œ CLIë¥¼ ë§Œë“¤ì—ˆë‹¤.</p><p><pre><code class="language-python">def main():
    if len(sys.argv) < 2:
        print_help()
        return
    
    command = sys.argv[1].lower()
    
    if command == "news":
        scrape_news()
    elif command == "interactive":
        interactive_mode()
    elif command == "help":
        print_help()</code></pre></p><p><h4><strong>ëŒ€í™”í˜• ëª¨ë“œ êµ¬í˜„</strong></h4></p><p><pre><code class="language-python">def interactive_mode():
    print("ğŸ¤– Web Scraper Interactive Mode")
    scraper = WebScraper(delay=1.0)
    
    while True:
        command = input("\nscraper> ").strip().lower()
        
        if command == "news":
            headlines = scraper.scrape_news_headlines(NEWS_SITES[:2])
            print(f"Collected {len(headlines)} headlines")
        elif command == "summary":
            summary = scraper.get_data_summary()
            print(f"Total items: {summary['total_items']}")
        elif command == "quit":
            break</code></pre></p><p><h3><strong>4ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ì‘ì„± (10ë¶„)</strong></h3></p><p>Day 1ë³´ë‹¤ ë” ì²´ê³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í–ˆë‹¤.</p><p><pre><code class="language-python">@patch('src.web_scraper.scraper.requests.Session.get')
def test_fetch_page_success(self, mock_get, scraper, mock_html):
    """í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ í…ŒìŠ¤íŠ¸"""
    mock_response = Mock()
    mock_response.content = mock_html.encode('utf-8')
    mock_get.return_value = mock_response
    
    soup = scraper.fetch_page("https://example.com")
    
    assert isinstance(soup, BeautifulSoup)
    assert soup.find('h1').get_text() == "Test Headline 1"</code></pre></p><p><strong>í…ŒìŠ¤íŠ¸ ê²°ê³¼:</strong> 11ê°œ í…ŒìŠ¤íŠ¸ ëª¨ë‘ í†µê³¼! âœ…</p><p>---</p><p><h2>ğŸ› ë§Œë‚œ ë¬¸ì œë“¤ê³¼ í•´ê²° ê³¼ì •</h2></p><p><h3><strong>ë¬¸ì œ 1: CSS ì„ íƒìê°€ ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¦„</strong></h3></p><p><strong>ì›ì¸:</strong> ê° ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë§ˆë‹¤ HTML êµ¬ì¡°ê°€ ì™„ì „íˆ ë‹¤ë¦„</p><p><strong>í•´ê²°:</strong> ì„¤ì • íŒŒì¼ë¡œ ì‚¬ì´íŠ¸ë³„ ì„ íƒìë¥¼ ë¶„ë¦¬í•˜ê³ , ì—¬ëŸ¬ ì„ íƒìë¥¼ ì‹œë„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„</p><p><pre><code class="language-python">'selector': '.storylink, .titleline > a'  # ì—¬ëŸ¬ ì„ íƒìë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„</code></pre></p><p><h3><strong>ë¬¸ì œ 2: ìƒëŒ€ URL vs ì ˆëŒ€ URL</strong></h3></p><p><strong>ì›ì¸:</strong> ì–´ë–¤ ì‚¬ì´íŠ¸ëŠ” <code>/article/123</code> í˜•íƒœ, ì–´ë–¤ ì‚¬ì´íŠ¸ëŠ” <code>https://...</code> í˜•íƒœ</p><p><strong>í•´ê²°:</strong> <code>urljoin()</code>ìœ¼ë¡œ ìë™ ë³€í™˜</p><p><pre><code class="language-python">if link and not link.startswith('http'):
    link = urljoin(site['url'], link)</code></pre></p><p><h3><strong>ë¬¸ì œ 3: ì‚¬ì´íŠ¸ ì ‘ê·¼ ì°¨ë‹¨</strong></h3></p><p><strong>ì›ì¸:</strong> ì¼ë¶€ ì‚¬ì´íŠ¸ì—ì„œ ë´‡ìœ¼ë¡œ ì¸ì‹í•´ì„œ ì°¨ë‹¨</p><p><strong>í•´ê²°:</strong> 
<ul><li>User-Agent í—¤ë” ì„¤ì •</li><li>ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ ì¶”ê°€</li><li>Session ì¬ì‚¬ìš©ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¸Œë¼ìš°ì§• ì‹œë®¬ë ˆì´ì…˜</li></ul></p><p><h3><strong>ë¬¸ì œ 4: í…ŒìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ HTTP ìš”ì²­</strong></h3></p><p><strong>ì›ì¸:</strong> í…ŒìŠ¤íŠ¸ ì¤‘ ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì— ìš”ì²­ì„ ë³´ë‚´ë©´ ëŠë¦¬ê³  ë¶ˆì•ˆì •</p><p><strong>í•´ê²°:</strong> <code>unittest.mock.patch</code>ë¡œ HTTP ìš”ì²­ì„ ëª¨í‚¹</p><p>---</p><p><h2>ğŸ‰ ì™„ì„±ëœ ê²°ê³¼ë¬¼</h2></p><p><h3><strong>ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘</strong></h3></p><p><pre><code class="language-bash">$ uv run python -m src.web_scraper news</p><p>ğŸ” Scraping major news sites...</p><p>âœ… Found 15 headlines
1. [Hacker News] Show HN: I built a Python web scraper
2. [BBC News] Tech industry shows strong growth
3. [Reuters] Market updates and analysis</p><p>ğŸ’¾ Data saved to: news_headlines_20240123_140530.csv</p><p>ğŸ“Š Summary:
  Hacker News: 8 articles
  BBC News: 3 articles
  Reuters: 2 articles
  CNN: 2 articles</code></pre></p><p><h3><strong>ëŒ€í™”í˜• ëª¨ë“œ</strong></h3></p><p><pre><code class="language-">ğŸ¤– Web Scraper Interactive Mode
Commands: news, tech, summary, clear, quit</p><p>scraper> news
Collected 10 headlines
scraper> summary
Total items: 10
  Hacker News: 6
  BBC News: 4
scraper> save
Data saved to: interactive_scrape_20240123_141205.csv</code></pre></p><p><h3><strong>CSV ì¶œë ¥ ì˜ˆì‹œ</strong></h3></p><p>| title | link | source | scraped_at |
|-------|------|--------|------------|
| Show HN: I built a web scraper | https://news.ycombinator.com/item?id=123 | Hacker News | 2024-01-23T14:05:30 |
| Tech industry shows growth | https://www.bbc.com/news/technology-123 | BBC News | 2024-01-23T14:05:32 |</p><p>---</p><p><h2>ğŸ’¡ ë°°ìš´ ì ë“¤</h2></p><p><h3><strong>ê¸°ìˆ ì  ì¸¡ë©´</strong></h3></p><p>1. <strong>ì›¹ ìŠ¤í¬ë˜í•‘ì˜ í˜„ì‹¤</strong>: ê° ì‚¬ì´íŠ¸ë§ˆë‹¤ êµ¬ì¡°ê°€ ë‹¤ë¥´ê³  ê³„ì† ë³€ê²½ë¨
2. <strong>ì˜ˆì˜ ë°”ë¥¸ ìŠ¤í¬ë˜í•‘</strong>: ì§€ì—° ì‹œê°„, User-Agent, robots.txt ì¤€ìˆ˜
3. <strong>ë°ì´í„° ì •ê·œí™”</strong>: ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ ì¼ê´€ëœ í˜•íƒœë¡œ ë³€í™˜
4. <strong>ì—ëŸ¬ ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±</strong>: ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ, ì‚¬ì´íŠ¸ ë³€ê²½ ë“±ì— ëŒ€ë¹„
5. <strong>í…ŒìŠ¤íŠ¸ ì „ëµ</strong>: ì™¸ë¶€ ì˜ì¡´ì„±ì„ ëª¨í‚¹í•˜ëŠ” ë°©ë²•</p><p><h3><strong>ê°œë°œ í”„ë¡œì„¸ìŠ¤ ì¸¡ë©´</strong></h3></p><p>1. <strong>ì„¤ì • ë¶„ë¦¬</strong>: ì½”ë“œì™€ ë°ì´í„°(ì„ íƒì)ë¥¼ ë¶„ë¦¬í•´ì„œ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
2. <strong>ì ì§„ì  ê°œë°œ</strong>: ê¸°ë³¸ ê¸°ëŠ¥ë¶€í„° ì‹œì‘í•´ì„œ ë‹¨ê³„ì ìœ¼ë¡œ í™•ì¥
3. <strong>ì‚¬ìš©ì ê²½í—˜</strong>: CLI ì¸í„°í˜ì´ìŠ¤ì—ì„œ í”¼ë“œë°±ê³¼ ì§„í–‰ìƒí™© í‘œì‹œ
4. <strong>ë°ì´í„° ì¤‘ì‹¬ ì‚¬ê³ </strong>: ìˆ˜ì§‘-ì²˜ë¦¬-ì €ì¥-ë¶„ì„ì˜ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•</p><p><h3><strong>ìœ¤ë¦¬ì  ì¸¡ë©´</strong></h3></p><p>1. <strong>í•©ë²•ì  ìŠ¤í¬ë˜í•‘</strong>: ê³µê°œëœ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë§Œ ëŒ€ìƒ
2. <strong>ì„œë²„ ë¶€í•˜ ìµœì†Œí™”</strong>: ì ì ˆí•œ ì§€ì—° ì‹œê°„ê³¼ ìš”ì²­ ì œí•œ
3. <strong>ì €ì‘ê¶Œ ì¡´ì¤‘</strong>: í—¤ë“œë¼ì¸ê³¼ ë§í¬ë§Œ ìˆ˜ì§‘, ì „ì²´ ë‚´ìš©ì€ ìˆ˜ì§‘í•˜ì§€ ì•ŠìŒ</p><p>---</p><p><h2>ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°</h2></p><p>ìµœì¢… ì™„ì„±ëœ í”„ë¡œì íŠ¸ êµ¬ì¡°:</p><p><pre><code class="language-">day02-web-scraper/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ web_scraper/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ __main__.py         # íŒ¨í‚¤ì§€ ì§„ì…ì 
â”‚       â”œâ”€â”€ main.py             # CLI ì¸í„°í˜ì´ìŠ¤
â”‚       â”œâ”€â”€ scraper.py          # í•µì‹¬ ìŠ¤í¬ë˜í¼ ì—”ì§„
â”‚       â””â”€â”€ news_sites.py       # ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì„¤ì •
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_scraper.py         # í¬ê´„ì  í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
â”œâ”€â”€ pyproject.toml              # uv í”„ë¡œì íŠ¸ ì„¤ì •
â”œâ”€â”€ uv.lock                     # ì˜ì¡´ì„± ì ê¸ˆ íŒŒì¼
â”œâ”€â”€ README.md                   # í”„ë¡œì íŠ¸ ë¬¸ì„œ
â””â”€â”€ BLOG.md                     # ì´ íŒŒì¼</code></pre></p><p>---</p><p><h2>ğŸ”¥ ì•ìœ¼ë¡œì˜ ê³„íš</h2></p><p><h3><strong>Day 3 ì˜ˆê³ : ì‘ì—… ê´€ë¦¬ ì‹œìŠ¤í…œ</strong></h3>
<ul><li>SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™</li><li>CRUD ê¸°ëŠ¥ì´ ìˆëŠ” TODO ì•±</li><li>Rich ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì˜ˆìœ í„°ë¯¸ë„ UI</li><li>ë°ì´í„° ë°±ì—…/ë³µì› ê¸°ëŠ¥</li></ul></p><p><h3><strong>ì›¹ ìŠ¤í¬ë˜í¼ í–¥í›„ ê°œì„  ì‚¬í•­</strong></h3>
<ul><li>[ ] ë” ë§ì€ ë‰´ìŠ¤ ì†ŒìŠ¤ ì¶”ê°€</li><li>[ ] í‚¤ì›Œë“œ í•„í„°ë§ ê¸°ëŠ¥</li><li>[ ] ìŠ¤ì¼€ì¤„ë§ìœ¼ë¡œ ìë™ ìˆ˜ì§‘</li><li>[ ] ë°ì´í„° ì‹œê°í™” (matplotlib)</li><li>[ ] ì›¹ ì¸í„°í˜ì´ìŠ¤ (Flask)</li></ul></p><p>---</p><p><h2>ğŸ“Š ì„±ê³¼ ì¸¡ì •</h2></p><p><h3><strong>ê°œë°œ ì‹œê°„ ë‹¨ì¶•</strong></h3>
<ul><li><strong>Day 1</strong>: í”„ë¡œì íŠ¸ ì…‹ì—… 10ë¶„</li><li><strong>Day 2</strong>: í”„ë¡œì íŠ¸ ì…‹ì—… 15ë¶„ (ë” ë³µì¡í•œ ì˜ì¡´ì„±)</li><li><strong>í–¥ìƒ</strong>: uv ì‚¬ìš©ë²•ì— ìµìˆ™í•´ì ¸ ì „ì²´ì ìœ¼ë¡œ ë” ë¹¨ë¼ì§</li></ul></p><p><h3><strong>ì½”ë“œ í’ˆì§ˆ í–¥ìƒ</strong></h3>
<ul><li><strong>í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€</strong>: 11ê°œ í…ŒìŠ¤íŠ¸ë¡œ í•µì‹¬ ê¸°ëŠ¥ ëª¨ë‘ ì»¤ë²„</li><li><strong>ì—ëŸ¬ ì²˜ë¦¬</strong>: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜, íŒŒì‹± ì˜¤ë¥˜ ë“± ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ ëŒ€ì‘</li><li><strong>ëª¨ë“ˆí™”</strong>: ê¸°ëŠ¥ë³„ë¡œ íŒŒì¼ì„ ë¶„ë¦¬í•´ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ</li></ul></p><p><h3><strong>ì‹¤ìš©ì„±</strong></h3>
<ul><li><strong>ì‹¤ì œ ë°ì´í„°</strong>: ê°€ì§œ ë°ì´í„°ê°€ ì•„ë‹Œ ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘</li><li><strong>ìœ ì—°ì„±</strong>: ìƒˆë¡œìš´ ì‚¬ì´íŠ¸ë¥¼ ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°</li><li><strong>í™•ì¥ì„±</strong>: ë‰´ìŠ¤ ì™¸ì— ë‹¤ë¥¸ ì½˜í…ì¸ ë„ ìŠ¤í¬ë˜í•‘ ê°€ëŠ¥</li></ul></p><p>---</p><p><h2>ğŸ¯ ë§ˆë¬´ë¦¬</h2></p><p><strong>60ë¶„ ë§Œì— ì‹¤ìš©ì ì¸ ì›¹ ìŠ¤í¬ë˜í¼ë¥¼ ì™„ì„±</strong>í–ˆë‹¤ëŠ” ê²Œ ë¿Œë“¯í•˜ë‹¤.</p><p>íŠ¹íˆ Day 1ì˜ ê³„ì‚°ê¸°ì™€ ë‹¬ë¦¬ ì´ë²ˆì—” <strong>ì‹¤ì œ ì¸í„°ë„·ì˜ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ”</strong> í”„ë¡œì íŠ¸ë¼ ë” í¥ë¯¸ë¡œì› ë‹¤. ê° ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ HTML êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê³ , ì•ˆì •ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê³¼ì •ì—ì„œ ì›¹ ê°œë°œì˜ í˜„ì‹¤ì ì¸ ì–´ë ¤ì›€ì„ ì²´í—˜í•  ìˆ˜ ìˆì—ˆë‹¤.</p><p>ë¬´ì—‡ë³´ë‹¤ <strong>"ë‚´ê°€ ë§Œë“  ë„êµ¬ë¡œ ì‹¤ì œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆë‹¤"</strong>ëŠ” ì„±ì·¨ê°ì´ í¬ë‹¤. ì´ì œ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ìë™ìœ¼ë¡œ ëª¨ì•„ì„œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ê±°ë‚˜, íŠ¹ì • í‚¤ì›Œë“œë¥¼ ì¶”ì í•  ìˆ˜ë„ ìˆë‹¤.</p><p><strong>Day 2ì—ì„œ ì´ ì •ë„ ì‹¤ë ¥ì´ë¼ë©´, 30ì¼ í›„ì—ëŠ” ì •ë§ ì–´ë–¤ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ë„ ë§Œë“¤ ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤!</strong> ğŸš€</p><p>---</p><p><h2>ğŸ“ ì†ŒìŠ¤ì½”ë“œ</h2></p><p>ì „ì²´ ì†ŒìŠ¤ì½”ë“œëŠ” GitHubì— ì˜¬ë ¤ë’€ë‹¤: <a href="https://github.com/Reasonofmoon/30-day-uv-python-challenge">30-day-uv-python-challenge</a></p><p><pre><code class="language-bash"><h1>ì§ì ‘ ì‹¤í–‰í•´ë³´ê³  ì‹¶ë‹¤ë©´</h1>
git clone https://github.com/Reasonofmoon/30-day-uv-python-challenge.git
cd 30-day-uv-python-challenge/day02-web-scraper
uv sync
uv run python -m src.web_scraper interactive</code></pre></p><p>---</p><p><strong>ë‹¤ìŒ í¬ìŠ¤íŒ…ì€ Day 3 - ì‘ì—… ê´€ë¦¬ ì‹œìŠ¤í…œ ê°œë°œê¸°ë¡œ ì°¾ì•„ëµ™ê² ìŠµë‹ˆë‹¤!</strong></p><p><strong>í•¨ê»˜ ì„±ì¥í•˜ëŠ” ê°œë°œìê°€ ë˜ì–´ìš”! ğŸ’ª</strong></p><p>---</p><p><em>#Python #uv #WebScraping #BeautifulSoup4 #pandas #ê°œë°œì¼ê¸° #30ì¼ì±Œë¦°ì§€ #ë‰´ìŠ¤ìŠ¤í¬ë˜í¼</em></p><p>---</p><p><strong>Created by <a href="https://v0-neobrutalist-ui-design-sigma-seven.vercel.app/">Reasonofmoon</a> | uv Python Developer Journey</strong></p>
        </main>
        
        <div class="meta-info">
            <h3>í”„ë¡œì íŠ¸ ì •ë³´</h3>
            <p><strong>ê¸°ìˆ  ìŠ¤íƒ:</strong> requests, beautifulsoup4, pandas, lxml</p>
            <p><strong>ê°œë°œ ì‹œê°„:</strong> 60ë¶„</p>
            <p><strong>í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€:</strong> 85%+</p>
            <p><strong>í”„ë¡œì íŠ¸ ë§í¬:</strong> <a href="https://github.com/Reasonofmoon/30-day-uv-python-challenge/tree/main/day02-web-scraper">GitHubì—ì„œ ë³´ê¸°</a></p>
        </div>
        
        <div class="tags">
            <span class="tag">#requests</span> <span class="tag">#beautifulsoup4</span> <span class="tag">#pandas</span> <span class="tag">#lxml</span> <span class="tag">#Python</span> <span class="tag">#uv</span> <span class="tag">#30ì¼ì±Œë¦°ì§€</span> <span class="tag">#ê°œë°œì¼ê¸°</span>
        </div>
        
        <footer class="footer">
            <p>30ì¼ uv Python ë§ˆìŠ¤í„° ì±Œë¦°ì§€ | 
            <a href="https://github.com/Reasonofmoon">Reasonofmoon</a> | 
            2025ë…„ 07ì›” 24ì¼</p>
            <p>Built with â¤ï¸ and Python</p>
        </footer>
    </div>
</body>
</html>